# -*- coding: utf-8 -*-
"""fake_real_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AolktMAtzlQSw-PF4GIVL5ZqilneIG7d

**Introduction**

The proliferation of fake news has emerged as a significant challenge in the digital era, impacting public opinion and decision-making processes. The detection of fake news requires effective and reliable machine learning models capable of distinguishing between legitimate and deceptive content across various domains [1]. This study explores the application of Naïve Bayes and Logistic Regression models for fake news detection, leveraging advanced text preprocessing and feature engineering techniques to improve classification accuracy. By comparing these approaches, we aim to identify the most efficient and robust methods for detecting fake news.

**Aim and Objectives**

**Aim:** The goal of this study is to design and evaluate a Naïve Bayes-based model for detecting fake news across a variety of topics and domains.

**Objectives:**


1.   To collect and preprocess a comprehensive dataset of news articles from diverse sources.
2.   To implement a Naïve Bayes classifier for identifying fake news effectively.
3.   To evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score.
4.   To compare the Naïve Bayes classifier's results with other machine learning models to assess its relative performance and reliability.
5.   To offer insights and recommendations for enhancing fake news detection methodologies in future research.
"""

# download all necessary libraries
import nltk
nltk.download('stopwords')

# importing all necessary libraries
from wordcloud import WordCloud

from nltk.corpus import stopwords
from sklearn.metrics import confusion_matrix,classification_report,ConfusionMatrixDisplay,accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

import re
import string
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the datasets containing true and fake news articles from CSV files [2]
true_df = pd.read_csv('True.csv') # Dataset of news articles labeled as true
fake_df = pd.read_csv('Fake.csv') # Dataset of news articles labeled as fake

# Add a new column 'label' to each dataset to indicate the type of news
true_df['label'] = "real"
fake_df['label'] = "fake"

# Combine the true and fake news datasets into a single DataFrame
news_df = pd.concat([true_df, fake_df], axis=0)

# Display summary information about the combined dataset
news_df.info()

# Check for missing values in the combined dataset
news_df.isnull().sum()

# Set plot style and size
sns.set_style("whitegrid")
plt.figure(figsize=(8, 6))

# Plot the countplot with enhancements
sns.countplot(
    x='label',
    hue='label',  # Explicitly set hue to the same variable as x
    data=news_df,
    palette='viridis',
    order=news_df['label'].value_counts().index,
    dodge=False  # Ensure single bars are displayed
)

# Add value annotations
for bar in plt.gca().patches:
    plt.gca().text(
        bar.get_x() + bar.get_width() / 2,  # Center of bar
        bar.get_height() - bar.get_height() * 0.1,  # Slightly below the top
        f"{int(bar.get_height())}",  # Label as an integer
        ha="center", va="center", color="white", fontsize=10
    )

# Add title and axis labels
plt.title('Distribution of Real and Fake News', fontsize=16, weight='bold')
plt.xlabel('Label (Real vs Fake)', fontsize=12)
plt.ylabel('Count', fontsize=12)

# Show the plot
plt.show()

# Precompute value counts
subject_counts = fake_df['subject'].value_counts()

# Set up the plot
plt.figure(figsize=(10, 6))  # Adjust figure size for better readability

# Use Seaborn's barplot with explicit hue assignment
sns.barplot(
    x=subject_counts.index,  # Categories
    y=subject_counts.values,  # Counts
    palette='husl',
    hue=subject_counts.index,  # Explicitly assign hue to x
    dodge=False,  # Disable dodging to keep bars aligned
    legend=False  # Disable the legend
)

# Add labels and title
plt.xlabel('Subject', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.title('Distribution of Subjects', fontsize=14, weight='bold')
plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate and align x-axis labels

# Annotate bar heights
for i, v in enumerate(subject_counts.values):
    plt.text(i, v + 0.5, str(v), ha='center', fontsize=10)

# Show the plot
plt.tight_layout()  # Adjust layout to prevent label overlap
plt.show()

# Select only the 'text' and 'label' columns for training and testing
df = news_df[['text','label']]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42)

def clean_text(text):

    # Compile regex patterns for better performance
    square_brackets_pattern = re.compile(r'\[.*?\]')
    links_pattern = re.compile(r'https?://\S+|www\.\S+')
    html_tags_pattern = re.compile(r'<.*?>')
    non_alphabetic_pattern = re.compile(r"[^a-zA-Z?.!,¿]+")
    words_with_numbers_pattern = re.compile(r'\w*\d\w*')

    # Apply the cleaning steps
    text = str(text).lower()
    text = square_brackets_pattern.sub('', text)
    text = links_pattern.sub('', text)
    text = html_tags_pattern.sub('', text)
    text = non_alphabetic_pattern.sub(' ', text)
    text = words_with_numbers_pattern.sub('', text)
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    text = text.strip()  # Remove leading/trailing whitespace
    return text

def remove_stopword(text):
    stop_words = set(stopwords.words('english'))  # Use a set for faster lookups
    words = text.split()  # Tokenize text
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)  # Return the cleaned text as a string

# Apply the cleaning functions
df.loc[:, 'text'] = df['text'].apply(lambda x: remove_stopword(clean_text(x)))

# Display the distribution of labels in the training and testing sets
y_train.value_counts(), y_test.value_counts()

"""**Initial Testing and Model Comparison**

Our initial testing focused on Naïve Bayes and Logistic Regression models, with various hyperparameter settings explored to enhance performance. Specifically, for Naïve Bayes, we implemented three distinct approaches to improve feature representation:


*   Using CountVectorizer: Text data was transformed into a matrix of token counts for training.
*   Using TF-IDF: Term Frequency-Inverse Document Frequency was employed to weigh terms based on their importance across the dataset.
*   Using TF-IDF with Bi-Grams: TF-IDF was extended to include combinations of two consecutive words, capturing contextual relationships.
"""

# create the pipline
pipe = Pipeline([('cv', CountVectorizer()),
                ('svc', MultinomialNB())])

# fit the pipeline
pipe.fit(X_train, y_train)

# Function to evaluate and display performance metrics
def evaluate_model(pipeline, X, y, dataset_name="Dataset"):
    # Generate predictions
    preds = pipeline.predict(X)

    # Print the classification report
    print("\nClassification Report:")
    print(classification_report(y, preds))

    # Plot the confusion matrix using from_estimator
    fig, ax = plt.subplots(figsize=(8, 6))  # Set figure size
    disp = ConfusionMatrixDisplay.from_estimator(
        pipeline,
        X,
        y,
        cmap='Blues',  # Custom colormap for visualization
        ax=ax,
        colorbar=True,  # Add colorbar for enhanced interpretation
        display_labels=pipeline.classes_  # Display class labels
    )

    # Customize the plot
    ax.set_title(f'Confusion Matrix for {dataset_name}', fontsize=16, weight='bold')
    ax.set_xlabel('Predicted Labels', fontsize=12)
    ax.set_ylabel('True Labels', fontsize=12)
    ax.tick_params(axis='both', which='major', labelsize=10)

    # Adjust layout to ensure all elements fit properly
    plt.tight_layout()
    plt.show()

# Evaluate on the test set
evaluate_model(pipe, X_test, y_test, dataset_name="Test Set")

# create the pipline
pipe = Pipeline([('tfidf', TfidfVectorizer()),
                ('svc', MultinomialNB())])

# Fit the pipeline on the training data
pipe.fit(X_train, y_train)

# Function to evaluate and plot the model performance
def evaluate_model(pipeline, X, y, dataset_name="Dataset"):
    # Predict on the given dataset
    preds = pipeline.predict(X)

    # Print the classification report
    print("\nClassification Report:")
    print(classification_report(y, preds))

    # Plot the confusion matrix with customized settings
    fig, ax = plt.subplots(figsize=(8, 6))  # Set the figure size as needed
    disp = ConfusionMatrixDisplay.from_estimator(
        pipeline,
        X,
        y,
        cmap='Blues',  # Choose a colormap
        ax=ax,
        colorbar=True,  # Add colorbar for better visualization
        display_labels=pipeline.classes_  # Display class labels
    )

    # Add title and axis labels
    ax.set_title(f'Confusion Matrix for {dataset_name}', fontsize=16, weight='bold')
    ax.set_xlabel('Predicted Labels', fontsize=12)
    ax.set_ylabel('True Labels', fontsize=12)

    plt.show()

# Evaluate on the test set
evaluate_model(pipe, X_test, y_test, dataset_name="Test Set")

# Define the pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),  # TF-IDF vectorizer with bigrams
    ('clf', MultinomialNB())                        # Multinomial Naive Bayes classifier
])

# Fit the pipeline to the training data
pipeline.fit(X_train, y_train)

# Function to evaluate the model and plot results
def evaluate_model(pipeline, X, y, dataset_name="Dataset"):
    # Predict on the given dataset
    y_pred = pipeline.predict(X)

    # Print the classification report
    print("\nClassification Report:")
    print(classification_report(y, y_pred))

    # Generate confusion matrix
    conf_matrix = confusion_matrix(y, y_pred)

    # Plot the confusion matrix using seaborn heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True, xticklabels=pipeline.classes_, yticklabels=pipeline.classes_)
    plt.title(f'Confusion Matrix for {dataset_name}', fontsize=16)
    plt.xlabel('Predicted Labels', fontsize=12)
    plt.ylabel('True Labels', fontsize=12)
    plt.show()

# evaluate on test data
evaluate_model(pipeline, X_test, y_test, dataset_name="Test Set")

# Create the pipeline with BernoulliNB
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), binary=True)),  # TF-IDF with binary features
    ('clf', BernoulliNB())  # Bernoulli Naive Bayes
])

# Fit the pipeline to the training data
pipeline.fit(X_train, y_train)

# Function to evaluate the model and plot results
def evaluate_model(pipeline, X, y, dataset_name="Dataset"):
    # Predict on the given dataset
    y_pred = pipeline.predict(X)

    # Print the classification report
    print("\nClassification Report:")
    print(classification_report(y, y_pred))

    # Generate confusion matrix
    conf_matrix = confusion_matrix(y, y_pred)

    # Plot the confusion matrix using seaborn heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True,
                xticklabels=pipeline.classes_, yticklabels=pipeline.classes_)
    plt.title(f'Confusion Matrix for {dataset_name}', fontsize=16)
    plt.xlabel('Predicted Labels', fontsize=12)
    plt.ylabel('True Labels', fontsize=12)
    plt.show()

# evaluate on test data
test_accuracy = evaluate_model(pipeline, X_test, y_test, dataset_name="Test Set")

# Create the pipeline
pipe = Pipeline([
    ('cv', CountVectorizer()),  # Convert text data to feature vectors
    ('lr', LogisticRegression())  # Logistic Regression classifier
])

# Fit the pipeline
pipe.fit(X_train, y_train)

# Define the evaluation function
def evaluate_model(pipeline, X, y, dataset_name="Dataset"):
    # Generate predictions
    preds = pipeline.predict(X)

    # Print the classification report
    print("\nClassification Report:")
    print(classification_report(y, preds))

    # Plot the confusion matrix using from_estimator
    fig, ax = plt.subplots(figsize=(8, 6))  # Set figure size
    disp = ConfusionMatrixDisplay.from_estimator(
        pipeline,
        X,
        y,
        cmap='Blues',  # Custom colormap for visualization
        ax=ax,
        colorbar=True,  # Add colorbar for enhanced interpretation
        display_labels=pipeline.classes_  # Display class labels
    )

    # Customize the plot
    ax.set_title(f'Confusion Matrix for {dataset_name}', fontsize=16, weight='bold')
    ax.set_xlabel('Predicted Labels', fontsize=12)
    ax.set_ylabel('True Labels', fontsize=12)
    ax.tick_params(axis='both', which='major', labelsize=10)

    # Adjust layout to ensure all elements fit properly
    plt.tight_layout()
    plt.show()

# Evaluate the pipeline on the test set
evaluate_model(pipe, X_test, y_test, dataset_name="Test Set")

"""**Technical Analysis**

The core method used was the Naïve Bayes classifier due to its simplicity, efficiency, and proven capability in text classification tasks. Variations of Multinomial Naïve Bayes were tested with different feature extraction methods. Results demonstrated that TF-IDF with Bi-Grams achieved the best performance, with a weighted F1-score of 96%, surpassing both CountVectorizer and TF-IDF alone.

Additionally, Logistic Regression was tested with CountVectorizer, yielding exceptional results with a weighted F1-score of 100%, showcasing near-perfect classification accuracy.

**Evaluation Metrics**

Performance was evaluated using accuracy, precision, recall, and F1-score to ensure a comprehensive assessment of the models’ classification abilities:



*   CountVectorizer (MultinomialNB): Achieved a weighted F1-score of 95%, demonstrating strong performance in identifying both fake and real news.
*   TF-IDF (MultinomialNB): Delivered a weighted F1-score of 94%, showing slightly lower performance compared to CountVectorizer.
*   TF-IDF with Bi-Grams (MultinomialNB): Improved outcomes significantly, achieving a weighted F1-score of 96%, indicating the effectiveness of capturing contextual relationships.
*   TF-IDF with Bi-Grams (BernoulliNB): Also performed well, with a weighted F1-score of 96%, balancing precision and recall effectively.
*   CountVectorizer (Logistic Regression): Excelled with a weighted F1-score of 100%, demonstrating the highest accuracy and reliability among all tested methods.

**Conclusion**

The results highlight the strengths of TF-IDF with Bi-Grams and Logistic Regression with CountVectorizer in fake news detection. While Naïve Bayes provides an efficient baseline, Logistic Regression’s near-perfect performance underscores its suitability for this classification task, making it a promising approach for future enhancements.

**References**

[1]	H. Al Ibraheemi and M. Jabardi, “A Detecting Fake News Using Machine Learning: A Comparative Study of Techniques,” Journal of Kufa for Mathematics and Computer, vol. 11, no. 2, pp. 113–120, Aug. 2024, doi: 10.31642/JoKMC/2018/110213.

[2]     https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset/data
"""